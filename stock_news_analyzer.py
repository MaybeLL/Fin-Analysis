#!/usr/bin/env python3
"""
è‚¡ç¥¨æ–°é—»åˆ†æžå·¥å…·
å®šæ—¶æ‹‰å–ç›®æ ‡è‚¡ç¥¨å…¬å¸ç›¸å…³æ–°é—»ï¼Œå¹¶è¿›è¡Œæƒ…æ„Ÿåˆ†æž
"""

import json
import sqlite3
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any
from dataclasses import dataclass
import schedule
import re
import requests


@dataclass
class NewsItem:
    """æ–°é—»é¡¹ç›®æ•°æ®ç»“æž„"""
    title: str
    content: str
    url: str
    publish_time: datetime
    source: str
    sentiment_score: float = 0.0
    sentiment_label: str = "neutral"


class NewsAnalyzer:
    """æ–°é—»åˆ†æžå™¨"""
    
    def __init__(self, use_finbert: bool = True, cache_dir: str = "./models"):
        self.finbert_model = None
        self.finbert_tokenizer = None
        self.use_finbert = use_finbert
        self.cache_dir = cache_dir
        self.analysis_method = "auto"  # auto, finbert, lightweight
        
        if self.use_finbert:
            self._init_finbert()
    
    def _init_finbert(self):
        """åˆå§‹åŒ–FinBERTæ¨¡åž‹"""
        try:
            # æµ‹è¯•åŸºæœ¬æ¨¡å—å¯¼å…¥
            import torch
            print("âœ“ torchå¯¼å…¥æˆåŠŸ")
            
            import transformers
            print(f"âœ“ transformerså¯¼å…¥æˆåŠŸï¼Œç‰ˆæœ¬: {transformers.__version__}")
            
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            print("âœ“ æ ¸å¿ƒtransformersç±»å¯¼å…¥æˆåŠŸ")
            
            # æµ‹è¯•æ‰€æœ‰å¿…éœ€çš„å­æ¨¡å—
            test_modules = [
                'transformers.models',
                'transformers.models.bert',
                'transformers.models.bert.modeling_bert',
                'transformers.models.bert.configuration_bert',
                'transformers.models.bert.tokenization_bert'
            ]
            
            for module_name in test_modules:
                try:
                    __import__(module_name)
                    print(f"âœ“ {module_name} å¯ç”¨")
                except ImportError as e:
                    print(f"âš  {module_name} ä¸å¯ç”¨: {e}")
            
            import os
            
            # åˆ›å»ºç¼“å­˜ç›®å½•
            os.makedirs(self.cache_dir, exist_ok=True)
            
            model_name = "ProsusAI/finbert"
            print("æ­£åœ¨åŠ è½½FinBERTæ¨¡åž‹...")
            print("é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡åž‹æ–‡ä»¶ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            # å°è¯•åŠ è½½æ¨¡åž‹ï¼Œè®¾ç½®æ›´å®½æ¾çš„å‚æ•°
            self.finbert_tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                cache_dir=self.cache_dir,
                trust_remote_code=False,
                local_files_only=False
            )
            print("âœ“ FinBERT tokenizeråŠ è½½æˆåŠŸ")
            
            self.finbert_model = AutoModelForSequenceClassification.from_pretrained(
                model_name, 
                cache_dir=self.cache_dir,
                trust_remote_code=False,
                local_files_only=False,
                ignore_mismatched_sizes=True
            )
            print("âœ“ FinBERTæ¨¡åž‹åŠ è½½æˆåŠŸ")
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.finbert_model.eval()
            print("ðŸŽ‰ FinBERTå®Œå…¨åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError as e:
            print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨å¤‡ç”¨æƒ…æ„Ÿåˆ†æžæ–¹æ³•")
            self.finbert_model = None
            self.finbert_tokenizer = None
        except Exception as e:
            print(f"âŒ FinBERTæ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨å¤‡ç”¨æƒ…æ„Ÿåˆ†æžæ–¹æ³•")
            self.finbert_model = None
            self.finbert_tokenizer = None
    
    def _can_import(self, module_name):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¯¼å…¥æ¨¡å—"""
        try:
            __import__(module_name)
            return True
        except ImportError:
            return False
    
    def set_analysis_method(self, method: str):
        """è®¾ç½®åˆ†æžæ–¹æ³•"""
        if method in ["auto", "finbert", "lightweight"]:
            self.analysis_method = method
            print(f"åˆ†æžæ–¹æ³•å·²è®¾ç½®ä¸º: {method}")
        else:
            print(f"æ— æ•ˆçš„åˆ†æžæ–¹æ³•: {method}")
    
    def get_analysis_method(self) -> str:
        """èŽ·å–å½“å‰åˆ†æžæ–¹æ³•"""
        return self.analysis_method
    
    def analyze_sentiment_with_finbert(self, text: str) -> tuple[float, str]:
        """ä½¿ç”¨FinBERTè¿›è¡Œæƒ…æ„Ÿåˆ†æž"""
        try:
            import torch
            
            # æˆªå–æ–‡æœ¬é•¿åº¦ä»¥é€‚åº”æ¨¡åž‹è¾“å…¥é™åˆ¶
            max_length = 512
            if len(text) > max_length:
                text = text[:max_length]
            
            # ç¼–ç æ–‡æœ¬
            inputs = self.finbert_tokenizer(text, return_tensors="pt", 
                                          truncation=True, padding=True, max_length=max_length)
            
            # é¢„æµ‹
            with torch.no_grad():
                outputs = self.finbert_model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            
            # FinBERTè¾“å‡º: [negative, neutral, positive]
            neg_score = predictions[0][0].item()
            neu_score = predictions[0][1].item()
            pos_score = predictions[0][2].item()
            
            # è®¡ç®—æžæ€§åˆ†æ•° (-1 åˆ° 1)
            polarity = pos_score - neg_score
            
            # ç¡®å®šæ ‡ç­¾
            if pos_score > max(neg_score, neu_score):
                label = "positive"
            elif neg_score > max(pos_score, neu_score):
                label = "negative"
            else:
                label = "neutral"
            
            return polarity, label
            
        except Exception as e:
            print(f"FinBERTåˆ†æžå¤±è´¥: {e}")
            return self.analyze_sentiment_fallback(text)
    
    def analyze_sentiment_fallback(self, text: str) -> tuple[float, str]:
        """å¤‡ç”¨æƒ…æ„Ÿåˆ†æžæ–¹æ³•"""
        try:
            from textblob import TextBlob
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            
            if polarity > 0.1:
                label = "positive"
            elif polarity < -0.1:
                label = "negative"  
            else:
                label = "neutral"
                
            return polarity, label
            
        except ImportError:
            # ä½¿ç”¨è½»é‡çº§é‡‘èžæƒ…æ„Ÿåˆ†æžå™¨
            return self._lightweight_financial_analysis(text)
    
    def _lightweight_financial_analysis(self, text: str) -> tuple[float, str]:
        """è½»é‡çº§é‡‘èžæƒ…æ„Ÿåˆ†æž"""
        import re
        
        # é‡‘èžæ­£é¢è¯æ±‡
        positive_words = {
            'bullish', 'bull', 'rally', 'surge', 'soar', 'climb', 'rise', 'gain', 'up',
            'growth', 'profit', 'earnings', 'beat', 'exceed', 'outperform', 'strong',
            'robust', 'solid', 'healthy', 'positive', 'optimistic', 'confident',
            'breakthrough', 'success', 'expansion', 'milestone', 'record', 'high',
            'upgrade', 'buy', 'overweight', 'recommend', 'target', 'upside'
        }
        
        # é‡‘èžè´Ÿé¢è¯æ±‡
        negative_words = {
            'bearish', 'bear', 'crash', 'plunge', 'plummet', 'fall', 'drop', 'decline',
            'loss', 'losses', 'down', 'weak', 'poor', 'disappointing', 'miss', 'below',
            'underperform', 'concern', 'worry', 'fear', 'risk', 'threat', 'challenge',
            'pressure', 'struggle', 'difficulty', 'problem', 'issue', 'negative',
            'pessimistic', 'cautious', 'downgrade', 'sell', 'underweight', 'avoid'
        }
        
        # é«˜æƒé‡è¯æ±‡
        high_weight_positive = {'surge', 'soar', 'skyrocket', 'breakthrough', 'record', 'beat', 'exceed'}
        high_weight_negative = {'crash', 'plunge', 'plummet', 'collapse', 'devastating', 'disaster'}
        
        text_lower = text.lower()
        words = re.findall(r'\b\w+\b', text_lower)
        
        if not words:
            return 0.0, "neutral"
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        positive_score = 0
        negative_score = 0
        
        for word in words:
            if word in positive_words:
                weight = 2.0 if word in high_weight_positive else 1.0
                positive_score += weight
            elif word in negative_words:
                weight = 2.0 if word in high_weight_negative else 1.0
                negative_score += weight
        
        # ç™¾åˆ†æ¯”å’Œæ•°å­—åŠ æƒ
        if re.search(r'\d+%', text):
            if positive_score > negative_score:
                positive_score *= 1.3
            elif negative_score > positive_score:
                negative_score *= 1.3
        
        if re.search(r'\$\d+|\d+\.\d+[BMK]?', text):
            positive_score *= 1.1
            negative_score *= 1.1
        
        # å¤„ç†å¦å®šè¯
        negation_patterns = [r'\bnot\s+(\w+)', r'\bno\s+(\w+)', r'\bnever\s+(\w+)']
        for pattern in negation_patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if match in positive_words:
                    positive_score -= 1.5
                    negative_score += 1.0
        
        # å½’ä¸€åŒ–åˆ†æ•°
        total_words = len(words)
        normalized_positive = positive_score / total_words
        normalized_negative = negative_score / total_words
        
        polarity = (normalized_positive - normalized_negative) * 2
        polarity = max(-1.0, min(1.0, polarity))
        
        # ç¡®å®šæ ‡ç­¾
        if polarity > 0.15:
            label = "positive"
        elif polarity < -0.15:
            label = "negative"
        else:
            label = "neutral"
        
        return polarity, label
    
    def analyze_sentiment(self, text: str) -> tuple[float, str]:
        """åˆ†æžæ–‡æœ¬æƒ…æ„Ÿ - æ”¯æŒç®—æ³•é€‰æ‹©"""
        method = self.analysis_method
        
        # å¦‚æžœé€‰æ‹©FinBERTä½†ä¸å¯ç”¨ï¼Œåˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if method == "finbert":
            if self.finbert_model is not None and self.finbert_tokenizer is not None:
                return self.analyze_sentiment_with_finbert(text)
            else:
                print("FinBERTä¸å¯ç”¨ï¼Œä½¿ç”¨è½»é‡çº§åˆ†æž")
                return self._lightweight_financial_analysis(text)
        
        # å¦‚æžœé€‰æ‹©è½»é‡çº§ï¼Œç›´æŽ¥ä½¿ç”¨è½»é‡çº§åˆ†æž
        elif method == "lightweight":
            return self._lightweight_financial_analysis(text)
        
        # è‡ªåŠ¨æ¨¡å¼ï¼šä¼˜å…ˆFinBERTï¼Œä¸å¯ç”¨åˆ™ä½¿ç”¨è½»é‡çº§
        else:  # method == "auto"
            if self.finbert_model is not None and self.finbert_tokenizer is not None:
                return self.analyze_sentiment_with_finbert(text)
            else:
                return self._lightweight_financial_analysis(text)
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        try:
            from textblob import TextBlob
            blob = TextBlob(text)
            words = [word.lower() for word in blob.words if len(word) > 3]
            return list(set(words))
        except ImportError:
            # ç®€å•çš„å…³é”®è¯æå–å›žé€€æ–¹æ¡ˆ
            import re
            words = re.findall(r'\b\w{4,}\b', text.lower())
            return list(set(words))


class NewsDatabase:
    """æ–°é—»æ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, db_path: str = "stock_news.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                stock_symbol TEXT NOT NULL,
                title TEXT NOT NULL,
                content TEXT,
                url TEXT UNIQUE,
                source TEXT,
                publish_time DATETIME,
                sentiment_score REAL,
                sentiment_label TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def save_news(self, stock_symbol: str, news: NewsItem):
        """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO news 
                (stock_symbol, title, content, url, source, publish_time, sentiment_score, sentiment_label)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (stock_symbol, news.title, news.content, news.url, news.source,
                  news.publish_time, news.sentiment_score, news.sentiment_label))
            
            conn.commit()
        except sqlite3.Error as e:
            print(f"æ•°æ®åº“é”™è¯¯: {e}")
        finally:
            conn.close()
    
    def get_recent_news(self, stock_symbol: str, days: int = 7) -> List[Dict]:
        """èŽ·å–æœ€è¿‘å‡ å¤©çš„æ–°é—»"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        start_date = datetime.now() - timedelta(days=days)
        cursor.execute('''
            SELECT * FROM news 
            WHERE stock_symbol = ? AND publish_time >= ?
            ORDER BY publish_time DESC
        ''', (stock_symbol, start_date))
        
        columns = [description[0] for description in cursor.description]
        results = [dict(zip(columns, row)) for row in cursor.fetchall()]
        
        conn.close()
        return results


class NewsCollector:
    """æ–°é—»æ”¶é›†å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # æ ¹æ®é…ç½®åˆå§‹åŒ–åˆ†æžå™¨
        use_finbert = self.config.get("analysis", {}).get("use_finbert", True)
        cache_dir = self.config.get("analysis", {}).get("model_cache_dir", "./models")
        
        self.analyzer = NewsAnalyzer(use_finbert=use_finbert, cache_dir=cache_dir)
        self.database = NewsDatabase()
    
    def fetch_alpha_vantage_news(self, symbol: str, api_key: str) -> List[NewsItem]:
        """ä»ŽAlpha VantageèŽ·å–æ–°é—»"""
        url = f"https://www.alphavantage.co/query"
        params = {
            'function': 'NEWS_SENTIMENT',
            'tickers': symbol,
            'apikey': api_key,
            'limit': 50
        }
        
        news_items = []
        try:
            response = requests.get(url, params=params, timeout=30)
            data = response.json()
            
            if 'feed' in data:
                for item in data['feed']:
                    try:
                        publish_time = datetime.strptime(item.get('time_published', ''), '%Y%m%dT%H%M%S')
                    except:
                        publish_time = datetime.now()
                    
                    news_item = NewsItem(
                        title=item.get('title', ''),
                        content=item.get('summary', ''),
                        url=item.get('url', ''),
                        publish_time=publish_time,
                        source=item.get('source', 'Alpha Vantage')
                    )
                    
                    # åˆ†æžæƒ…æ„Ÿ
                    sentiment_score, sentiment_label = self.analyzer.analyze_sentiment(
                        news_item.title + " " + news_item.content
                    )
                    news_item.sentiment_score = sentiment_score
                    news_item.sentiment_label = sentiment_label
                    
                    news_items.append(news_item)
                    
        except Exception as e:
            print(f"èŽ·å–Alpha Vantageæ–°é—»å¤±è´¥: {e}")
        
        return news_items
    
    def fetch_finnhub_news(self, symbol: str, api_key: str) -> List[NewsItem]:
        """ä»ŽFinnhubèŽ·å–æ–°é—»"""
        url = "https://finnhub.io/api/v1/company-news"
        from_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        to_date = datetime.now().strftime('%Y-%m-%d')
        
        params = {
            'symbol': symbol,
            'from': from_date,
            'to': to_date,
            'token': api_key
        }
        
        news_items = []
        try:
            response = requests.get(url, params=params, timeout=30)
            data = response.json()
            
            for item in data:
                news_item = NewsItem(
                    title=item.get('headline', ''),
                    content=item.get('summary', ''),
                    url=item.get('url', ''),
                    publish_time=datetime.fromtimestamp(item.get('datetime', 0)),
                    source=item.get('source', 'Finnhub')
                )
                
                # åˆ†æžæƒ…æ„Ÿ
                sentiment_score, sentiment_label = self.analyzer.analyze_sentiment(
                    news_item.title + " " + news_item.content
                )
                news_item.sentiment_score = sentiment_score
                news_item.sentiment_label = sentiment_label
                
                news_items.append(news_item)
                
        except Exception as e:
            print(f"èŽ·å–Finnhubæ–°é—»å¤±è´¥: {e}")
        
        return news_items


class StockNewsAnalyzer:
    """è‚¡ç¥¨æ–°é—»åˆ†æžä¸»ç±»"""
    
    def __init__(self, config_file: str = "config.json"):
        self.config = self.load_config(config_file)
        self.collector = None
        self.running = False
        self.analyzer = NewsAnalyzer()  # åˆå§‹åŒ–åˆ†æžå™¨
    
    def load_config(self, config_file: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            # åˆ›å»ºé»˜è®¤é…ç½®
            default_config = {
                "stocks": ["AAPL", "GOOGL", "MSFT", "TSLA"],
                "api_keys": {
                    "alpha_vantage": "YOUR_ALPHA_VANTAGE_API_KEY",
                    "finnhub": "YOUR_FINNHUB_API_KEY"
                },
                "schedule": {
                    "interval_hours": 4,
                    "start_time": "09:00"
                },
                "analysis": {
                    "sentiment_threshold": 0.1,
                    "max_news_per_stock": 50
                }
            }
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            
            print(f"å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_file}")
            print("è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶å¹¶æ·»åŠ æ‚¨çš„APIå¯†é’¥")
            return default_config
    
    def set_analysis_method(self, method: str):
        """è®¾ç½®åˆ†æžæ–¹æ³•"""
        if hasattr(self, 'analyzer'):
            self.analyzer.set_analysis_method(method)
        else:
            print("åˆ†æžå™¨æœªåˆå§‹åŒ–")
    
    def get_analysis_method(self) -> str:
        """èŽ·å–å½“å‰åˆ†æžæ–¹æ³•"""
        if hasattr(self, 'analyzer'):
            return self.analyzer.get_analysis_method()
        return "auto"
    
    def collect_news_for_stock(self, symbol: str):
        """ä¸ºå•ä¸ªè‚¡ç¥¨æ”¶é›†æ–°é—»"""
        print(f"å¼€å§‹æ”¶é›† {symbol} çš„æ–°é—»...")
        
        # ç¡®ä¿collectorå­˜åœ¨
        if not self.collector:
            self.collector = NewsCollector(self.config)
        
        # è®¾ç½®åˆ†æžæ–¹æ³•
        if hasattr(self, 'analyzer'):
            self.collector.analyzer.set_analysis_method(self.analyzer.get_analysis_method())
        
        all_news = []
        
        # ä»ŽAlpha Vantageæ”¶é›†
        if self.config["api_keys"]["alpha_vantage"] != "YOUR_ALPHA_VANTAGE_API_KEY":
            alpha_news = self.collector.fetch_alpha_vantage_news(
                symbol, self.config["api_keys"]["alpha_vantage"]
            )
            all_news.extend(alpha_news)
        
        # ä»ŽFinnhubæ”¶é›†
        if self.config["api_keys"]["finnhub"] != "YOUR_FINNHUB_API_KEY":
            finnhub_news = self.collector.fetch_finnhub_news(
                symbol, self.config["api_keys"]["finnhub"]
            )
            all_news.extend(finnhub_news)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        for news in all_news:
            self.collector.database.save_news(symbol, news)
        
        print(f"å·²ä¸º {symbol} æ”¶é›†å¹¶ä¿å­˜äº† {len(all_news)} æ¡æ–°é—»")
        return all_news
    
    def run_collection_cycle(self):
        """è¿è¡Œä¸€æ¬¡å®Œæ•´çš„æ–°é—»æ”¶é›†å‘¨æœŸ"""
        self.collector = NewsCollector(self.config)
        # è®¾ç½®åˆ†æžæ–¹æ³•
        if hasattr(self, 'analyzer'):
            self.collector.analyzer.set_analysis_method(self.analyzer.get_analysis_method())
        
        for symbol in self.config["stocks"]:
            self.collect_news_for_stock(symbol)
            time.sleep(1)  # é¿å…APIé™æµ
    
    def generate_analysis_report(self, symbol: str, days: int = 7) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†æžæŠ¥å‘Š"""
        database = NewsDatabase()
        recent_news = database.get_recent_news(symbol, days)
        
        if not recent_news:
            return {"error": f"æ²¡æœ‰æ‰¾åˆ° {symbol} æœ€è¿‘ {days} å¤©çš„æ–°é—»"}
        
        # ç»Ÿè®¡åˆ†æž
        total_news = len(recent_news)
        positive_news = len([n for n in recent_news if n['sentiment_label'] == 'positive'])
        negative_news = len([n for n in recent_news if n['sentiment_label'] == 'negative'])
        neutral_news = total_news - positive_news - negative_news
        
        avg_sentiment = sum([n['sentiment_score'] for n in recent_news]) / total_news
        
        report = {
            "symbol": symbol,
            "period_days": days,
            "total_news": total_news,
            "sentiment_distribution": {
                "positive": positive_news,
                "negative": negative_news,
                "neutral": neutral_news
            },
            "average_sentiment_score": round(avg_sentiment, 3),
            "overall_sentiment": "positive" if avg_sentiment > 0.1 else "negative" if avg_sentiment < -0.1 else "neutral",
            "recent_headlines": [n['title'] for n in recent_news[:5]]
        }
        
        return report
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        interval_hours = self.config["schedule"]["interval_hours"]
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval_hours).hours.do(self.run_collection_cycle)
        
        print(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯ {interval_hours} å°æ—¶è¿è¡Œä¸€æ¬¡")
        print("æŒ‰ Ctrl+C åœæ­¢ç¨‹åº")
        
        self.running = True
        try:
            while self.running:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            print("\nç¨‹åºå·²åœæ­¢")
            self.running = False
    
    def run_once(self):
        """è¿è¡Œä¸€æ¬¡æ–°é—»æ”¶é›†"""
        self.run_collection_cycle()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è‚¡ç¥¨æ–°é—»åˆ†æžå·¥å…·")
    parser.add_argument("--config", default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--once", action="store_true", help="è¿è¡Œä¸€æ¬¡åŽé€€å‡º")
    parser.add_argument("--report", type=str, help="ç”ŸæˆæŒ‡å®šè‚¡ç¥¨ä»£ç çš„åˆ†æžæŠ¥å‘Š")
    parser.add_argument("--days", type=int, default=7, help="æŠ¥å‘Šåˆ†æžçš„å¤©æ•°")
    
    args = parser.parse_args()
    
    analyzer = StockNewsAnalyzer(args.config)
    
    if args.report:
        # ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_analysis_report(args.report, args.days)
        print(json.dumps(report, indent=2, ensure_ascii=False))
    elif args.once:
        # è¿è¡Œä¸€æ¬¡
        analyzer.run_once()
    else:
        # å¯åŠ¨å®šæ—¶å™¨
        analyzer.start_scheduler()


if __name__ == "__main__":
    main()